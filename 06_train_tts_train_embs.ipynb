{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5159f4-435f-4992-afd1-4986e5b46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install transformers==4.33.3 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e843ceee-31a4-4fa0-b333-9529707897bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter_TTS, Prompter_TTS_Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cd55c3-f334-4b90-a4f7-8189285723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "# base_model: str = 'decapoda-research/llama-7b-hf' \n",
    "base_model: str = '../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/'\n",
    "# data_path: str = \"../HuggingFace-Download-Accelerator/hf_hub/datasets--yahma--alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "data_path: str = \"./datasets/tts_data_train_1.json\"\n",
    "# data_path: str = \"./datasets/tts_data_train_1_small.json\"\n",
    "output_dir: str = \"./result/lora-tts-1_128\"\n",
    "# training hyperparams\n",
    "# batch_size: int = 128\n",
    "batch_size: int = 64\n",
    "micro_batch_size: int = 8\n",
    "num_epochs: int = 10\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 2000\n",
    "# val_set_size: int = 300\n",
    "# lora hyperparams\n",
    "lora_r: int = 128\n",
    "lora_alpha: int = 128\n",
    "lora_dropout: float = 0.05\n",
    "# lora_target_modules: List[str] = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False masks out inputs in loss\n",
    "add_eos_token: bool = False\n",
    "group_by_length: bool = True  # faster but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "# resume_from_checkpoint: str = \"./result/lora-tts-1_256/checkpoint-30/\"\n",
    "prompt_template_name: str = \"alpaca_tts_pho_enc\"  # The prompt template to use will default to alpaca.\n",
    "encodec_dim: int = 1024\n",
    "# encodec_nq: int = int(os.path.basename(data_path).split(\".\")[0][-1])\n",
    "encodec_nq: int = 1\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        # data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"text\"], data_point[\"phonemes\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8a6b3a-9477-4461-9cd3-26393ea3e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/\n",
      "data_path: ./datasets/tts_data_train_1.json\n",
      "output_dir: ./result/lora-tts-1_128\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 128\n",
      "lora_alpha: 128\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca_tts_pho_enc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter_TTS_Phonemes(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed14b4f-8a37-4e40-a5a9-80f3226afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save= [\"base_model.model.model.embed_tokens.weight\", \"base_model.model.lm_head.weight\"],\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80932cb9-87bf-4fdc-b3aa-58ea1a120cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,536,768 || all params: 6,881,808,384 || trainable%: 5.892880844268506\n"
     ]
    }
   ],
   "source": [
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    \n",
    "    m = torch.load(resume_from_checkpoint + \"embedding_layer.pt\")\n",
    "    model.base_model.model.model.embed_tokens.load_state_dict(m)\n",
    "    m = torch.load(resume_from_checkpoint + \"lm_head.pt\")\n",
    "    model.base_model.model.lm_head.parameters(m)\n",
    "    \n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "else:\n",
    "    resume = False\n",
    "        \n",
    "        \n",
    "for parameter in model.base_model.model.model.embed_tokens.parameters():\n",
    "    parameter.requires_grad = True\n",
    "for parameter in model.base_model.model.lm_head.parameters():\n",
    "    parameter.requires_grad = True\n",
    "    \n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9298eda0-d949-4242-9376-e2ca248b21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class CustomSaveCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.save_steps == 0:\n",
    "            save_dir = output_dir + f\"/checkpoint-{state.global_step}/\"\n",
    "            torch.save(model.base_model.model.model.embed_tokens.state_dict(), save_dir + 'embedding_layer.pt')\n",
    "            torch.save(model.base_model.model.lm_head.state_dict(), save_dir + 'lm_head.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f118af-40ab-4d3b-b7e6-00c2365d4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38365/38365 [01:16<00:00, 502.06 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:04<00:00, 491.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    callbacks=[CustomSaveCallback()],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50 if val_set_size > 0 else None,\n",
    "        save_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c68582-53d1-4c44-89b7-f3d359cab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3559' max='5990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3559/5990 26:14:31 < 17:56:05, 0.04 it/s, Epoch 5.93/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.495200</td>\n",
       "      <td>7.619437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.152700</td>\n",
       "      <td>5.649225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.852000</td>\n",
       "      <td>5.683593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.665500</td>\n",
       "      <td>6.882706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.928700</td>\n",
       "      <td>5.511198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.903300</td>\n",
       "      <td>6.377765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.896900</td>\n",
       "      <td>5.979404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.860800</td>\n",
       "      <td>5.663955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.863300</td>\n",
       "      <td>6.728551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.878600</td>\n",
       "      <td>7.177281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.871100</td>\n",
       "      <td>6.275043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.956000</td>\n",
       "      <td>5.836938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.947800</td>\n",
       "      <td>5.704318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.039500</td>\n",
       "      <td>5.402270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.973700</td>\n",
       "      <td>5.381955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.963100</td>\n",
       "      <td>5.723631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.937500</td>\n",
       "      <td>5.401095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>5.001900</td>\n",
       "      <td>5.546538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.990900</td>\n",
       "      <td>5.615058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.966500</td>\n",
       "      <td>5.535338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.929100</td>\n",
       "      <td>5.656201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.963000</td>\n",
       "      <td>5.663929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.987000</td>\n",
       "      <td>5.613083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>5.006800</td>\n",
       "      <td>5.752831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.958500</td>\n",
       "      <td>5.680235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>5.023900</td>\n",
       "      <td>5.820460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>5.008700</td>\n",
       "      <td>5.718930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>5.081800</td>\n",
       "      <td>5.851686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>5.033600</td>\n",
       "      <td>5.745836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.019700</td>\n",
       "      <td>5.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>5.003000</td>\n",
       "      <td>5.837647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>5.015800</td>\n",
       "      <td>5.891260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>5.011900</td>\n",
       "      <td>5.879694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>5.007200</td>\n",
       "      <td>5.941327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>5.018500</td>\n",
       "      <td>5.938712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>5.140700</td>\n",
       "      <td>5.866663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>5.061800</td>\n",
       "      <td>5.968139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>5.115100</td>\n",
       "      <td>5.931865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>5.071300</td>\n",
       "      <td>5.994447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.098300</td>\n",
       "      <td>5.963629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>5.045100</td>\n",
       "      <td>6.022094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>5.126900</td>\n",
       "      <td>5.951698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>5.140200</td>\n",
       "      <td>6.013556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>5.099100</td>\n",
       "      <td>6.049005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>5.057100</td>\n",
       "      <td>6.059976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.108900</td>\n",
       "      <td>6.083159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>5.101300</td>\n",
       "      <td>6.106084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.248700</td>\n",
       "      <td>6.146534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>5.117300</td>\n",
       "      <td>6.195318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.193000</td>\n",
       "      <td>6.194869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>5.220700</td>\n",
       "      <td>6.240437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>5.214300</td>\n",
       "      <td>6.163563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>5.141400</td>\n",
       "      <td>6.217369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>5.214000</td>\n",
       "      <td>6.658217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>5.060100</td>\n",
       "      <td>6.226380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>5.080600</td>\n",
       "      <td>6.093021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>5.050800</td>\n",
       "      <td>6.114713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>5.009800</td>\n",
       "      <td>6.074358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>5.027400</td>\n",
       "      <td>6.172665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.056800</td>\n",
       "      <td>6.087417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>5.068700</td>\n",
       "      <td>6.098595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>5.018400</td>\n",
       "      <td>6.069278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>5.008100</td>\n",
       "      <td>6.094399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>5.041000</td>\n",
       "      <td>6.132751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>5.101700</td>\n",
       "      <td>6.162609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>5.088800</td>\n",
       "      <td>6.160698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>5.018600</td>\n",
       "      <td>6.149428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>5.042200</td>\n",
       "      <td>6.125754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>5.045400</td>\n",
       "      <td>6.188825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.025300</td>\n",
       "      <td>6.165398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>5.130200</td>\n",
       "      <td>6.222850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\n",
    "    \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "186e28d6-a828-432f-9a34-a90070a46634",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c693fff-e361-47fb-a1dc-5199327880fc",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "# print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d9539e9-f8a7-467b-8cc5-0d15f976880b",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f313933-3862-421a-8a8b-6a0016e2dfc4",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        # data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9b6eb16-f54a-45df-90e3-e2085e0b0edc",
   "metadata": {},
   "source": [
    "a = prompt[278:344]\n",
    "code = tokenizer(a, add_special_tokens=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a064a364-9a5f-4e7b-950a-d5a2f41891c5",
   "metadata": {},
   "source": [
    "a = prompt[346:]\n",
    "code = tokenizer(a)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f483bf-16d0-42c5-8722-36b3fc54c26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
