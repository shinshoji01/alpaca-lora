{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5159f4-435f-4992-afd1-4986e5b46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install transformers==4.33.3 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e843ceee-31a4-4fa0-b333-9529707897bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from utils.prompter import Prompter_TTS_Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cd55c3-f334-4b90-a4f7-8189285723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "# base_model: str = 'decapoda-research/llama-7b-hf' \n",
    "base_model: str = '../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/'\n",
    "# data_path: str = \"../HuggingFace-Download-Accelerator/hf_hub/datasets--yahma--alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "# train_path: str = \"./datasets/tts_data_train_1.json\"\n",
    "train_path: str = \"./datasets/tts_data_train_1_small.json\"\n",
    "test_path: str = \"./datasets/tts_data_test_1.json\"\n",
    "output_dir: str = \"./result/lora-tts-1_phoenc\"\n",
    "# training hyperparams\n",
    "# batch_size: int = 128\n",
    "batch_size: int = 64\n",
    "micro_batch_size: int = 8\n",
    "num_epochs: int = 10\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 2000\n",
    "# val_set_size: int = 300\n",
    "# lora hyperparams\n",
    "lora_r: int = 16\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "# lora_target_modules: List[str] = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False masks out inputs in loss\n",
    "add_eos_token: bool = False\n",
    "group_by_length: bool = True  # faster but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "# resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "resume_from_checkpoint: str = \"./result/lora-tts-1_phoenc/checkpoint-5850/\"\n",
    "prompt_template_name: str = \"alpaca_tts_pho_enc\"  # The prompt template to use will default to alpaca.\n",
    "encodec_dim: int = 1024\n",
    "# encodec_nq: int = int(os.path.basename(data_path).split(\".\")[0][-1])\n",
    "encodec_nq: int = 1\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"phonemes\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8a6b3a-9477-4461-9cd3-26393ea3e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/\n",
      "output_dir: ./result/lora-tts-1_phoenc\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 16\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'embed_tokens', 'lm_head']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: ./result/lora-tts-1_phoenc/checkpoint-5850/\n",
      "prompt template: alpaca_tts_pho_enc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        # f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter_TTS_Phonemes(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed14b4f-8a37-4e40-a5a9-80f3226afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s]\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 33111. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting from ./result/lora-tts-1_phoenc/checkpoint-5850/adapter_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:01<00:00, 376.45 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 393.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "_ = tokenizer.add_tokens([\"[SAUDIO]\", \"[EAUDIO]\"], special_tokens=True)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "_ = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "_ = tokenizer.add_tokens([\"[SPHONE]\", \"[EPHONE]\"], special_tokens=True)\n",
    "new_phonemes = [f\"<{t}>\" for t in  phonemes.split(\"\\n\")]\n",
    "_ = tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save= [\"base_model.model.model.embed_tokens.weight\", \"base_model.model.lm_head.weight\"],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "train_data = load_dataset(\"json\", data_files=train_path)\n",
    "test_data = load_dataset(\"json\", data_files=test_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "        \n",
    "model.eval()\n",
    "train_data = train_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "test_data = test_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5228e6-bdc6-4d92-a894-cc893e286a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phonemes:\n",
      "<PHN><DH><AH0><HH><OW1><L><K><AA2><N><V><ER0><S><EY1><SH><AH0><N><R><AE1><N><AA1><N><DH><AH0><B><R><EH1><K><F><AH0><S><T><W><IH1><CH><W><AH1><N><AH0><N><D><AO1><L><AH0><B><Y><UW1><Z><D><R><AW1><N><D><L><IY0></PHN>\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_point = test_data[1]\n",
    "device = \"cuda\"\n",
    "temperature = 1.0\n",
    "top_p = 0.75\n",
    "top_k = 100\n",
    "num_beams = 4\n",
    "max_new_tokens = 128\n",
    "\n",
    "prompt = prompter.generate_prompt(\n",
    "    # data_point[\"text\"],\n",
    "    data_point[\"phonemes\"],\n",
    "    # data_point[\"output\"],\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    num_beams=num_beams,\n",
    "    # **kwargs,\n",
    ")\n",
    "generate_params = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"generation_config\": generation_config,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d77ba53-58ce-4e97-928a-0d3d2ad1aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>### Phonemes:\n",
      "<PHN><DH><AH0><HH><OW1><L><K><AA2><N><V><ER0><S><EY1><SH><AH0><N><R><AE1><N><AA1><N><DH><AH0><B><R><EH1><K><F><AH0><S><T><W><IH1><CH><W><AH1><N><AH0><N><D><AO1><L><AH0><B><Y><UW1><Z><D><R><AW1><N><D><L><IY0></PHN>\n",
      "\n",
      "### Response:\n",
      "<SPCH> <121> <408> <408> <491> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <310> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724> <724>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
