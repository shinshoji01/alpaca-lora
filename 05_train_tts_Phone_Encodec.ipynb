{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5159f4-435f-4992-afd1-4986e5b46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install transformers==4.33.3 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fcd727e-70b5-49a0-bf59-451c6e3dee1e",
   "metadata": {},
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "_ = tokenizer.add_tokens([\"[SAUDIO]\", \"[EAUDIO]\"], special_tokens=True)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "_ = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "_ = tokenizer.add_tokens([\"[SPHONE]\", \"[EPHONE]\"], special_tokens=True)\n",
    "new_phonemes = [f\"<{t}>\" for t in  phonemes.split(\"\\n\")]\n",
    "_ = tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "data_point = data[\"train\"][0]\n",
    "prompt = prompter.generate_prompt(\n",
    "    data_point[\"text\"],\n",
    "    data_point[\"phonemes\"],\n",
    "    data_point[\"output\"],\n",
    ")\n",
    "print(prompt)\n",
    "code = tokenizer(prompt)[\"input_ids\"]\n",
    "print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e843ceee-31a4-4fa0-b333-9529707897bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter_TTS, Prompter_TTS_Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cd55c3-f334-4b90-a4f7-8189285723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "# base_model: str = 'decapoda-research/llama-7b-hf' \n",
    "base_model: str = '../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/'\n",
    "# data_path: str = \"../HuggingFace-Download-Accelerator/hf_hub/datasets--yahma--alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "data_path: str = \"./datasets/tts_data_train_1.json\"\n",
    "# data_path: str = \"./datasets/tts_data_train_1_small.json\"\n",
    "output_dir: str = \"./result/lora-tts-1_phoenc\"\n",
    "# training hyperparams\n",
    "# batch_size: int = 128\n",
    "batch_size: int = 64\n",
    "micro_batch_size: int = 8\n",
    "num_epochs: int = 10\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 2000\n",
    "# val_set_size: int = 300\n",
    "# lora hyperparams\n",
    "lora_r: int = 16\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "# lora_target_modules: List[str] = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False masks out inputs in loss\n",
    "add_eos_token: bool = False\n",
    "group_by_length: bool = True  # faster but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "# resume_from_checkpoint: str = \"./result/lora-tts-1/checkpoint-12/\"\n",
    "prompt_template_name: str = \"alpaca_tts_pho_enc\"  # The prompt template to use will default to alpaca.\n",
    "encodec_dim: int = 1024\n",
    "# encodec_nq: int = int(os.path.basename(data_path).split(\".\")[0][-1])\n",
    "encodec_nq: int = 1\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        # data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"text\"], data_point[\"phonemes\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8a6b3a-9477-4461-9cd3-26393ea3e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/\n",
      "data_path: ./datasets/tts_data_train_1.json\n",
      "output_dir: ./result/lora-tts-1_phoenc\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 16\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'embed_tokens', 'lm_head']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca_tts_pho_enc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter_TTS_Phonemes(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed14b4f-8a37-4e40-a5a9-80f3226afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.03it/s]\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 33111. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save= [\"base_model.model.model.embed_tokens.weight\", \"base_model.model.lm_head.weight\"],\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80932cb9-87bf-4fdc-b3aa-58ea1a120cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,967,840 || all params: 6,765,484,768 || trainable%: 0.2655809689349374\n"
     ]
    }
   ],
   "source": [
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "        \n",
    "# for parameter in model.base_model.model.model.embed_tokens.parameters():\n",
    "#     parameter.requires_grad = True\n",
    "# for parameter in model.base_model.model.lm_head.parameters():\n",
    "#     parameter.requires_grad = True\n",
    "    \n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f118af-40ab-4d3b-b7e6-00c2365d4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38365/38365 [01:27<00:00, 439.55 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:04<00:00, 411.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50 if val_set_size > 0 else None,\n",
    "        save_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c68582-53d1-4c44-89b7-f3d359cab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5368' max='5990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5368/5990 54:26:16 < 6:18:36, 0.03 it/s, Epoch 8.95/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.555700</td>\n",
       "      <td>4.786286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.218400</td>\n",
       "      <td>4.412578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.642200</td>\n",
       "      <td>3.904494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.441000</td>\n",
       "      <td>3.173466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.349500</td>\n",
       "      <td>2.928165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.302700</td>\n",
       "      <td>2.835316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.237500</td>\n",
       "      <td>2.906141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.216600</td>\n",
       "      <td>2.731682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.202500</td>\n",
       "      <td>2.676429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.175600</td>\n",
       "      <td>2.629176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.144200</td>\n",
       "      <td>2.564658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>2.469910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.135500</td>\n",
       "      <td>2.476912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.118100</td>\n",
       "      <td>2.443335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.112300</td>\n",
       "      <td>2.432572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.108400</td>\n",
       "      <td>2.400264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.097100</td>\n",
       "      <td>2.381117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.102800</td>\n",
       "      <td>2.388326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.083500</td>\n",
       "      <td>2.435780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.071200</td>\n",
       "      <td>2.356718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.089300</td>\n",
       "      <td>2.369987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.069000</td>\n",
       "      <td>2.347199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.031300</td>\n",
       "      <td>2.372925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.051100</td>\n",
       "      <td>2.393339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.023400</td>\n",
       "      <td>2.339908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.024700</td>\n",
       "      <td>2.315516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.032700</td>\n",
       "      <td>2.337358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.032400</td>\n",
       "      <td>2.291956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.025200</td>\n",
       "      <td>2.307384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.023000</td>\n",
       "      <td>2.314827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.024800</td>\n",
       "      <td>2.298577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.027400</td>\n",
       "      <td>2.277999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.038000</td>\n",
       "      <td>2.305112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.002000</td>\n",
       "      <td>2.297348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.997900</td>\n",
       "      <td>2.278801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.011800</td>\n",
       "      <td>2.260543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.984600</td>\n",
       "      <td>2.242861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.978900</td>\n",
       "      <td>2.232590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.990300</td>\n",
       "      <td>2.238070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.983600</td>\n",
       "      <td>2.231055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.963900</td>\n",
       "      <td>2.231948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.964100</td>\n",
       "      <td>2.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.959600</td>\n",
       "      <td>2.208217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.959500</td>\n",
       "      <td>2.202318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.952600</td>\n",
       "      <td>2.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.938200</td>\n",
       "      <td>2.217307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.965900</td>\n",
       "      <td>2.200532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.959600</td>\n",
       "      <td>2.194237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.914000</td>\n",
       "      <td>2.184005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.926900</td>\n",
       "      <td>2.194466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.933100</td>\n",
       "      <td>2.172948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.915300</td>\n",
       "      <td>2.169504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.915900</td>\n",
       "      <td>2.169788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.923300</td>\n",
       "      <td>2.165894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.921600</td>\n",
       "      <td>2.160361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.914700</td>\n",
       "      <td>2.176213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.936500</td>\n",
       "      <td>2.155818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.928100</td>\n",
       "      <td>2.153848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.905200</td>\n",
       "      <td>2.157541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.896700</td>\n",
       "      <td>2.149747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.903200</td>\n",
       "      <td>2.144695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.882400</td>\n",
       "      <td>2.150875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.894200</td>\n",
       "      <td>2.147460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.857700</td>\n",
       "      <td>2.139564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.895200</td>\n",
       "      <td>2.153099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.876500</td>\n",
       "      <td>2.126075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>2.126944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.864900</td>\n",
       "      <td>2.123975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.861800</td>\n",
       "      <td>2.122419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.866800</td>\n",
       "      <td>2.111003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.862700</td>\n",
       "      <td>2.101121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.877600</td>\n",
       "      <td>2.088552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.844800</td>\n",
       "      <td>2.091759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.859900</td>\n",
       "      <td>2.077459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.841000</td>\n",
       "      <td>2.064902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.832800</td>\n",
       "      <td>2.070862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.854200</td>\n",
       "      <td>2.060575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.821400</td>\n",
       "      <td>2.059769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.836900</td>\n",
       "      <td>2.047190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.834600</td>\n",
       "      <td>2.039359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.828900</td>\n",
       "      <td>2.039461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.828500</td>\n",
       "      <td>2.027773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.824500</td>\n",
       "      <td>2.029035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.819800</td>\n",
       "      <td>2.019402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.781200</td>\n",
       "      <td>2.016946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.791800</td>\n",
       "      <td>2.017143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.798700</td>\n",
       "      <td>2.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.793300</td>\n",
       "      <td>2.011548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.786200</td>\n",
       "      <td>2.003274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.805400</td>\n",
       "      <td>2.001042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.802400</td>\n",
       "      <td>1.996465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.790400</td>\n",
       "      <td>1.996009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.785200</td>\n",
       "      <td>1.990843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.783800</td>\n",
       "      <td>1.992857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.789400</td>\n",
       "      <td>1.989277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.775300</td>\n",
       "      <td>1.984066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.796900</td>\n",
       "      <td>1.980038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.757700</td>\n",
       "      <td>1.977393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.747300</td>\n",
       "      <td>1.976820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.773900</td>\n",
       "      <td>1.972254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.772600</td>\n",
       "      <td>1.973491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>1.966717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.760200</td>\n",
       "      <td>1.968538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.763300</td>\n",
       "      <td>1.969050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>1.963938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.744200</td>\n",
       "      <td>1.963928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.760300</td>\n",
       "      <td>1.958454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\n",
    "    \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "186e28d6-a828-432f-9a34-a90070a46634",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c693fff-e361-47fb-a1dc-5199327880fc",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "# print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d9539e9-f8a7-467b-8cc5-0d15f976880b",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f313933-3862-421a-8a8b-6a0016e2dfc4",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        # data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9b6eb16-f54a-45df-90e3-e2085e0b0edc",
   "metadata": {},
   "source": [
    "a = prompt[278:344]\n",
    "code = tokenizer(a, add_special_tokens=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a064a364-9a5f-4e7b-950a-d5a2f41891c5",
   "metadata": {},
   "source": [
    "a = prompt[346:]\n",
    "code = tokenizer(a)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f483bf-16d0-42c5-8722-36b3fc54c26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
