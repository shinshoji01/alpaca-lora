{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5159f4-435f-4992-afd1-4986e5b46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install transformers==4.33.3 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e843ceee-31a4-4fa0-b333-9529707897bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter_TTS, Prompter_TTS_Phonemes, Prompter_Pretraining_Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cd55c3-f334-4b90-a4f7-8189285723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "# base_model: str = 'decapoda-research/llama-7b-hf' \n",
    "base_model: str = '../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/'\n",
    "# data_path: str = \"../HuggingFace-Download-Accelerator/hf_hub/datasets--yahma--alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "data_path: str = \"./datasets/tts_data_train_1.json\"\n",
    "# data_path: str = \"./datasets/tts_data_train_1_small.json\"\n",
    "output_dir: str = \"./result/lora-pretrain_1_128_enc\"\n",
    "# training hyperparams\n",
    "# batch_size: int = 128\n",
    "batch_size: int = 64\n",
    "micro_batch_size: int = 8\n",
    "num_epochs: int = 10\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 2000\n",
    "# val_set_size: int = 300\n",
    "# lora hyperparams\n",
    "lora_r: int = 128\n",
    "lora_alpha: int = 128\n",
    "lora_dropout: float = 0.05\n",
    "# lora_target_modules: List[str] = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False masks out inputs in loss\n",
    "add_eos_token: bool = False\n",
    "group_by_length: bool = True  # faster but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "# resume_from_checkpoint: str = \"./result/lora-tts-1_128_text/checkpoint-1800/\"\n",
    "prompt_template_name: str = \"alpaca_enc\"  # The prompt template to use will default to alpaca.\n",
    "encodec_dim: int = 1024\n",
    "# encodec_nq: int = int(os.path.basename(data_path).split(\".\")[0][-1])\n",
    "encodec_nq: int = 1\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        # data_point[\"text\"],\n",
    "        # data_point[\"text\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"text\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8a6b3a-9477-4461-9cd3-26393ea3e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/\n",
      "data_path: ./datasets/tts_data_train_1.json\n",
      "output_dir: ./result/lora-pretrain_1_128_enc\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 128\n",
      "lora_alpha: 128\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca_enc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter_Pretraining_Audio(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed14b4f-8a37-4e40-a5a9-80f3226afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save= [\"base_model.model.model.embed_tokens.weight\", \"base_model.model.lm_head.weight\"],\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80932cb9-87bf-4fdc-b3aa-58ea1a120cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,536,768 || all params: 6,881,808,384 || trainable%: 5.892880844268506\n"
     ]
    }
   ],
   "source": [
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    \n",
    "    m = torch.load(resume_from_checkpoint + \"embedding_layer.pt\")\n",
    "    model.base_model.model.model.embed_tokens.load_state_dict(m)\n",
    "    m = torch.load(resume_from_checkpoint + \"lm_head.pt\")\n",
    "    model.base_model.model.lm_head.parameters(m)\n",
    "    \n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "else:\n",
    "    resume = False\n",
    "        \n",
    "        \n",
    "for parameter in model.base_model.model.model.embed_tokens.parameters():\n",
    "    parameter.requires_grad = True\n",
    "for parameter in model.base_model.model.lm_head.parameters():\n",
    "    parameter.requires_grad = True\n",
    "    \n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9298eda0-d949-4242-9376-e2ca248b21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class CustomSaveCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.save_steps == 0:\n",
    "            save_dir = output_dir + f\"/checkpoint-{state.global_step}/\"\n",
    "            torch.save(model.base_model.model.model.embed_tokens.state_dict(), save_dir + 'embedding_layer.pt')\n",
    "            torch.save(model.base_model.model.lm_head.state_dict(), save_dir + 'lm_head.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f118af-40ab-4d3b-b7e6-00c2365d4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38365/38365 [01:06<00:00, 572.66 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:03<00:00, 566.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    callbacks=[CustomSaveCallback()],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50 if val_set_size > 0 else None,\n",
    "        save_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c68582-53d1-4c44-89b7-f3d359cab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2014' max='5990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2014/5990 14:19:59 < 28:19:27, 0.04 it/s, Epoch 3.36/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.306400</td>\n",
       "      <td>6.354631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.481100</td>\n",
       "      <td>5.733207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.714100</td>\n",
       "      <td>4.421762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.496500</td>\n",
       "      <td>3.877067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.512600</td>\n",
       "      <td>3.978732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.361400</td>\n",
       "      <td>3.954210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.380000</td>\n",
       "      <td>3.666457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.378200</td>\n",
       "      <td>3.598212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.398200</td>\n",
       "      <td>3.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.249300</td>\n",
       "      <td>3.676593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>562.206900</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>649735.500000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>49.185500</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>684247.100000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1229016.600000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>9769.042200</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.024600</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2510283.800000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>13880.946900</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2035674.600000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.179300</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>221.404800</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>14271.309400</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>30703.662500</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>38951.990600</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>75.142900</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>323593.400000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>9.995600</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.353800</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>620513.350000</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1140.527200</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>10.019400</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4155.235200</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>18.148100</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>8288.667200</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2622.462100</td>\n",
       "      <td>3.258812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\n",
    "    \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650a687-43c4-4a67-a156-17cccc93fbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
