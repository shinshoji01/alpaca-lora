{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5159f4-435f-4992-afd1-4986e5b46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install transformers==4.33.3 -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fcd727e-70b5-49a0-bf59-451c6e3dee1e",
   "metadata": {},
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "_ = tokenizer.add_tokens([\"[SAUDIO]\", \"[EAUDIO]\"], special_tokens=True)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "_ = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "_ = tokenizer.add_tokens([\"[SPHONE]\", \"[EPHONE]\"], special_tokens=True)\n",
    "new_phonemes = [f\"<{t}>\" for t in  phonemes.split(\"\\n\")]\n",
    "_ = tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "data_point = data[\"train\"][0]\n",
    "prompt = prompter.generate_prompt(\n",
    "    data_point[\"text\"],\n",
    "    data_point[\"phonemes\"],\n",
    "    data_point[\"output\"],\n",
    ")\n",
    "print(prompt)\n",
    "code = tokenizer(prompt)[\"input_ids\"]\n",
    "print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e843ceee-31a4-4fa0-b333-9529707897bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter_TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cd55c3-f334-4b90-a4f7-8189285723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "# base_model: str = 'decapoda-research/llama-7b-hf' \n",
    "base_model: str = '../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/'\n",
    "# data_path: str = \"../HuggingFace-Download-Accelerator/hf_hub/datasets--yahma--alpaca-cleaned/alpaca_data_cleaned.json\"\n",
    "data_path: str = \"./datasets/tts_data_train_1.json\"\n",
    "# data_path: str = \"./datasets/tts_data_train_1_small.json\"\n",
    "output_dir: str = \"./result/lora-tts-1\"\n",
    "# training hyperparams\n",
    "# batch_size: int = 128\n",
    "batch_size: int = 64\n",
    "micro_batch_size: int = 8\n",
    "num_epochs: int = 10\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 512\n",
    "val_set_size: int = 2000\n",
    "# val_set_size: int = 300\n",
    "# lora hyperparams\n",
    "lora_r: int = 16\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "# lora_target_modules: List[str] = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "lora_target_modules: List[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"embed_tokens\", \"lm_head\"]\n",
    "# llm hyperparams\n",
    "train_on_inputs: bool = True  # if False masks out inputs in loss\n",
    "add_eos_token: bool = False\n",
    "group_by_length: bool = True  # faster but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"\"\n",
    "wandb_run_name: str = \"\"\n",
    "wandb_watch: str = \"\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "# resume_from_checkpoint: str = \"./result/lora-tts-1/checkpoint-12/\"\n",
    "prompt_template_name: str = \"alpaca_tts\"  # The prompt template to use will default to alpaca.\n",
    "encodec_dim: int = 1024\n",
    "# encodec_nq: int = int(os.path.basename(data_path).split(\".\")[0][-1])\n",
    "encodec_nq: int = 1\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"text\"], data_point[\"phonemes\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8a6b3a-9477-4461-9cd3-26393ea3e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: ../HuggingFace-Download-Accelerator/hf_local/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/\n",
      "data_path: ./datasets/tts_data_train_1.json\n",
      "output_dir: ./result/lora-tts-1\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 10\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "val_set_size: 2000\n",
      "lora_r: 16\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'embed_tokens', 'lm_head']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca_tts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter_TTS(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed14b4f-8a37-4e40-a5a9-80f3226afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards:  48%|████▊     | 16/33 [00:10<00:09,  1.82it/s]"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "_ = tokenizer.add_tokens([\"[SAUDIO]\", \"[EAUDIO]\"], special_tokens=True)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "_ = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "_ = tokenizer.add_tokens([\"[SPHONE]\", \"[EPHONE]\"], special_tokens=True)\n",
    "new_phonemes = [f\"<{t}>\" for t in  phonemes.split(\"\\n\")]\n",
    "_ = tokenizer.add_tokens(new_phonemes)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "torch.manual_seed(42)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # modules_to_save= [\"base_model.model.model.embed_tokens.weight\", \"base_model.model.lm_head.weight\"],\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80932cb9-87bf-4fdc-b3aa-58ea1a120cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "        \n",
    "# for parameter in model.base_model.model.model.embed_tokens.parameters():\n",
    "#     parameter.requires_grad = True\n",
    "# for parameter in model.base_model.model.lm_head.parameters():\n",
    "#     parameter.requires_grad = True\n",
    "    \n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f118af-40ab-4d3b-b7e6-00c2365d4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38365/38365 [01:43<00:00, 370.14 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:05<00:00, 357.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50 if val_set_size > 0 else None,\n",
    "        save_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name=wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c68582-53d1-4c44-89b7-f3d359cab5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2201' max='5990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2201/5990 23:55:03 < 41:12:40, 0.03 it/s, Epoch 3.67/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.342800</td>\n",
       "      <td>4.617792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.189900</td>\n",
       "      <td>3.771241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.829700</td>\n",
       "      <td>2.462426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.614800</td>\n",
       "      <td>2.214964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.522000</td>\n",
       "      <td>2.053428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.491200</td>\n",
       "      <td>1.965355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.466300</td>\n",
       "      <td>1.913427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.466600</td>\n",
       "      <td>1.908595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.438200</td>\n",
       "      <td>1.853526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.417200</td>\n",
       "      <td>1.839996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.420900</td>\n",
       "      <td>1.877314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.393500</td>\n",
       "      <td>1.767936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.390300</td>\n",
       "      <td>1.766823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.381500</td>\n",
       "      <td>1.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.384200</td>\n",
       "      <td>1.722229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.354300</td>\n",
       "      <td>1.716334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.373100</td>\n",
       "      <td>1.725881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.353200</td>\n",
       "      <td>1.706881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.342900</td>\n",
       "      <td>1.708010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.344200</td>\n",
       "      <td>1.710989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.347300</td>\n",
       "      <td>1.689005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.337500</td>\n",
       "      <td>1.679449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.354100</td>\n",
       "      <td>1.664057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.338200</td>\n",
       "      <td>1.659855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.308800</td>\n",
       "      <td>1.668127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.316600</td>\n",
       "      <td>1.666655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.315300</td>\n",
       "      <td>1.654868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.315500</td>\n",
       "      <td>1.640325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.317800</td>\n",
       "      <td>1.650367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.309200</td>\n",
       "      <td>1.648599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.315300</td>\n",
       "      <td>1.642754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.287800</td>\n",
       "      <td>1.632202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.333000</td>\n",
       "      <td>1.630547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.308600</td>\n",
       "      <td>1.626223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.289200</td>\n",
       "      <td>1.627366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.285600</td>\n",
       "      <td>1.604258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.284200</td>\n",
       "      <td>1.604969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.297100</td>\n",
       "      <td>1.604481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.261100</td>\n",
       "      <td>1.609236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.278600</td>\n",
       "      <td>1.605203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.268700</td>\n",
       "      <td>1.603345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.264200</td>\n",
       "      <td>1.600702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.296500</td>\n",
       "      <td>1.590413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/250 02:53 < 02:29, 0.77 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\n",
    "    \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "186e28d6-a828-432f-9a34-a90070a46634",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c693fff-e361-47fb-a1dc-5199327880fc",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "# print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b44cff00-2d82-470f-9f83-bf5760a5f0c2",
   "metadata": {},
   "source": [
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<SPCH>\", \"</SPCH>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "new_tokens = [f\"<{t}>\" for t in range(encodec_dim*encodec_nq)]\n",
    "tokenizer.add_tokens(new_tokens, False)\n",
    "f = open(\"phoneme_vocab.txt\", \"r\")\n",
    "phonemes = f.read()\n",
    "special_tokens = {\"additional_special_tokens\": [\"<PHN>\", \"</PHN>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens, False)\n",
    "new_phonemes = [f\"<{t.upper()}>\" for t in  phonemes.split(\"\\n\")]\n",
    "tokenizer.add_tokens(new_phonemes);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae794506-27f0-4586-b0eb-4a0421ce7c3d",
   "metadata": {},
   "source": [
    "num = 0\n",
    "for idx in range(200):\n",
    "    data_point = data[\"train\"][idx]\n",
    "    prompt = prompter.generate_prompt(\n",
    "        data_point[\"text\"],\n",
    "        data_point[\"phonemes\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    # print(prompt)\n",
    "    code = tokenizer(prompt)[\"input_ids\"]\n",
    "    num += len(code)\n",
    "# print(tokenizer.decode(code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9b6eb16-f54a-45df-90e3-e2085e0b0edc",
   "metadata": {},
   "source": [
    "a = prompt[278:344]\n",
    "code = tokenizer(a, add_special_tokens=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a064a364-9a5f-4e7b-950a-d5a2f41891c5",
   "metadata": {},
   "source": [
    "a = prompt[346:]\n",
    "code = tokenizer(a)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f483bf-16d0-42c5-8722-36b3fc54c26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
